{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3AYxvET2YT2"
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**James**\n",
    "\n",
    "Cuando nos enfrentamos a un gran conjunto de variables correlacionadas, los componentes principales nos permiten resumir este conjunto con un número menor de variables representativas que, en conjunto, explican la mayor parte de la variabilidad del conjunto original. Las direcciones de los componentes principales son las direcciones en el espacio de características a lo largo de las cuales los datos originales son altamente variables. Estas direcciones también definen líneas y subespacios que están lo más cerca posible de la nube de datos. Para realizar la regresión de componentes principales, simplemente usamos componentes principales como predictores en un modelo de regresión en lugar del conjunto original más grande de variables.\n",
    "\n",
    "El análisis de componentes principales (PCA) se refiere al proceso mediante el cual se calculan los componentes principales y el uso posterior de estos componentes para comprender los datos. El PCA es un enfoque no supervisado, ya que involucra solo un conjunto de características $X_1, X_2, \\dots, X_M$ y ninguna respuesta asociada Y. Además de producir variables derivadas para su uso en problemas de aprendizaje supervisado, el PCA también sirve como una herramienta para la visualización de datos (visualización de las observaciones o visualización de las variables). También se puede utilizar como herramienta para la imputación de datos, es decir, para completar valores faltantes en una matriz de datos.\n",
    "\n",
    "Supongamos que deseamos visualizar $N$ observaciones con mediciones en un conjunto de $M$ características, $X_1, X_2, \\dots, X_M$, como parte de un análisis exploratorio de datos. Podríamos hacerlo examinando scatterplots bidimensionales de los datos, cada uno de los cuales contiene las mediciones de las $N$ observaciones en dos de las características. Sin embargo, existen $\\binom{M}{2} = M(M - 1)/2$ scatterplots de este tipo; por ejemplo, con $M = 10$ ¡hay 45 diagramas! Si $M$ es grande, entonces ciertamente no será posible observarlos todos; además, lo más probable es que ninguno de ellos sea informativo ya que cada uno contiene solo una pequeña fracción de la información total presente en el conjunto de datos. Claramente, se requiere un mejor método para visualizar las $N$ observaciones cuando $M$ es grande. En particular, nos gustaría encontrar una representación de baja dimensión de los datos que capture la mayor cantidad de información posible. Por ejemplo, si podemos obtener una representación bidimensional de los datos que capture la mayor parte de la información, entonces podemos representar gráficamente las observaciones en este espacio de baja dimensión. El PCA proporciona una herramienta para hacer precisamente esto. Busca una representación de baja dimensión de un conjunto de datos que contenga la mayor cantidad posible de variación. La idea es que cada una de las $N$ observaciones viva en un espacio de $M$ dimensiones, pero no todas estas dimensiones son igualmente interesantes. El PCA busca un pequeño número de dimensiones que sean lo más interesantes posible, donde el concepto de interés se mide por la cantidad en que varían las observaciones a lo largo de cada dimensión. Cada una de las dimensiones encontradas por el PCA es una combinación lineal de las $M$ características. Ahora explicamos la manera en que se encuentran estas dimensiones, o componentes principales. El primer componente principal de un conjunto de características $X_1, X_2, \\dots, X_M$ es la combinación lineal normalizada de las características\n",
    "\n",
    "$$\n",
    "Z_1 = \\varphi_{11}X_1 + \\varphi_{21}X_2 + \\dots + \\varphi_{M1}X_M\n",
    "$$\n",
    "\n",
    "que tiene la varianza más grande. Por normalizado, queremos decir que $\\sum_{j=1}^M\\varphi_{j1}^2 = 1$. Nos referimos a los elementos $\\varphi_{11}, \\varphi_{21}, \\dots + \\varphi_{M1}$ como los loadings del primer componente principal; en conjunto, los loadings forman el vector de cargas del componente principal, $\\varphi_1 = (\\varphi_{11}, \\varphi_{21}, \\dots + \\varphi_{M1})^T$. Restringimos los loadings de modo que su suma de cuadrados sea igual a uno, ya que de lo contrario, establecer que estos elementos sean arbitrariamente grandes en valor absoluto podría resultar en una varianza arbitrariamente grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVVglnoM60iuQb8Y59tHbX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
