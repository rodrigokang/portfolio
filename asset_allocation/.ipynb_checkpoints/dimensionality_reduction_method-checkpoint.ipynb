{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3AYxvET2YT2"
   },
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de componentes principales (PCA, por sus siglas en inglés) es una técnica estadística ampliamente utilizada para la reducción de dimensionalidad de conjuntos de datos. Esta técnica identifica las direcciones en las que los datos presentan mayor variabilidad y transforma los datos hacia un nuevo sistema de coordenadas basado en estas direcciones. Fue desarrollada por Karl Pearson en 1901 y posteriormente perfeccionada por Harold Hotelling en la década de 1930.\n",
    "\n",
    "El PCA es un método no supervisado, lo que significa que se basa únicamente en un conjunto de características $X_1, X_2, \\dots, X_M$ sin considerar ninguna respuesta asociada $Y$. Aunque su uso principal es para la reducción de dimensionalidad, también se aplica en problemas de aprendizaje supervisado, por ejemplo, utilizando las componentes principales como predictores en algoritmos de Machine Learning en lugar del conjunto original más grande de variables. Además, el PCA es una herramienta poderosa para la visualización de datos.\n",
    "\n",
    "El PCA transforma los datos en un nuevo sistema de coordenadas donde las componentes principales representan la mayor variabilidad en los datos. Estas componentes son secuencias de vectores unitarios que se ajustan de manera óptima a los datos, minimizando la distancia perpendicular promedio al cuadrado entre los puntos y la línea. Las direcciones de las componentes principales corresponden a aquellas en el espacio de características a lo largo de las cuales los datos muestran mayor variabilidad, definiendo líneas y subespacios que se aproximan lo más posible a la nube de datos.\n",
    "\n",
    "Una de las principales ventajas del PCA es su capacidad para resumir un conjunto grande de variables correlacionadas en un número menor de variables representativas que, en conjunto, explican la mayor parte de la variabilidad del conjunto original. Esto es especialmente útil en contextos con un gran número de variables, donde la visualización de los datos se torna un desafío. Por ejemplo, si se tienen $N$ observaciones y un conjunto de $M$ características $X_1, X_2, \\dots, X_M$, se podrían generar $\\binom{M}{2} = \\frac{M(M - 1)}{2}$ gráficos de dispersión (scatterplots) para visualizar los datos. Esto implica que, para $M = 10$, se tendrían 45 posibles gráficos de dispersión, lo que puede resultar impráctico si $M$ es grande. Además, es probable que ninguno de estos gráficos por sí solo sea informativo, ya que cada uno captura solo una fracción de la información total. Claramente, se requiere un método más eficiente para visualizar las $N$ observaciones cuando $M$ es grande, y aquí es donde el PCA resulta particularmente útil.\n",
    "\n",
    "El PCA permite encontrar una representación de baja dimensión de los datos que capture la mayor cantidad posible de variación. Aunque cada una de las $N$ observaciones vive en un espacio de $M$ dimensiones, no todas estas dimensiones son igualmente relevantes. El PCA identifica un pequeño número de dimensiones que son las más interesantes, donde el grado de interés se mide por la cantidad de variación en las observaciones a lo largo de cada dimensión. Cada una de estas dimensiones es una combinación lineal de las $M$ características originales.\n",
    "\n",
    "Por lo tanto, PCA proporciona una solución eficaz para la reducción de dimensionalidad y la visualización de datos en un espacio de baja dimensión. Por ejemplo, si se pueden derivar dos componentes principales que capturen la mayor parte de la variabilidad en los datos, las observaciones pueden representarse en un gráfico de dispersión bidimensional, facilitando así la identificación de patrones y grupos de datos relacionados entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receta general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que sigue explicamos de manera conceptual el procedimiento general mediante el cual se encuentran las componentes principales. En primer lugar, en PCA, las variables deben estar centradas para que tengan una media de cero. Además, los resultados obtenidos cuando realizamos el PCA también dependerán de si las variables han sido escaladas individualmente (cada una multiplicada por una constante diferente). El primer componente principal de un conjunto de características $X_1, X_2, \\dots, X_M$ es la combinación lineal normalizada de las características\n",
    "\n",
    "$$\n",
    "Z_1 = \\varphi_{11}X_1 + \\varphi_{21}X_2 + \\dots + \\varphi_{M1}X_M\n",
    "$$\n",
    "\n",
    "que tiene la varianza más grande. Por normalizado, queremos decir que $\\sum_{j=1}^M\\varphi_{j1}^2 = 1$. Nos referimos a los elementos $\\varphi_{11}, \\varphi_{21}, \\dots, \\varphi_{M1}$ como los loadings del primer componente principal; en conjunto, los loadings forman el vector de cargas del componente principal, $\\varphi_1 = (\\varphi_{11}, \\varphi_{21}, \\dots, \\varphi_{M1})^T$. Restringimos los loadings de modo que su suma de cuadrados sea igual a uno, ya que de lo contrario, establecer que estos elementos sean arbitrariamente grandes en valor absoluto podría resultar en una varianza arbitrariamente grande.\n",
    "\n",
    "Dado un conjunto de datos $N \\times M$ $\\textbf{X}$, ¿cómo calculamos el primer componente principal? Dado que solo nos interesa la varianza, suponemos que cada una de las variables en $\\textbf{X}$ se ha centrado para que tenga una media cero (es decir, las medias de las columnas de $\\textbf{X}$ son cero). Luego buscamos la combinación lineal de los valores de las características de la muestra de la forma\n",
    "\n",
    "$$\n",
    "z_{i1} = \\varphi_{11}x_{i1} + \\varphi_{21}x_{i2} + \\dots + \\varphi_{M1}x_{iM}\n",
    "$$\n",
    "\n",
    "que tiene la mayor varianza de muestra, sujeta a la restricción de que $\\sum_{j=1}^M \\varphi_{j1}^2 = 1$\n",
    "\n",
    "En otras palabras, el primer vector de carga del componente principal resuelve el problema de optimización\n",
    "\n",
    "$$\n",
    "\\max_{\\varphi_{11}, \\dots, \\varphi_{M1}} \\left[ \\frac{1}{N} \\sum_{i=1}^N \\left( \\sum_{j=1}^M \\varphi_{j1}x_{ij} \\right) \\right], \\,\\,\\, \\text{sujeto a} \\,\\,\\, \\sum_{j=1}^M \\varphi_{j1}^2 = 1\n",
    "$$\n",
    "\n",
    "A partir de la ecuación anterior, podemos escribir el objetivo como $\\frac{1}{N}\\sum_{i=1}^Nz_{i1}^2$. Dado que $\\frac{1}{N}\\sum_{i=1}^Nx_{ij} = 0$, el promedio de $z_{11}, \\dots, z_{N1}$ también será cero. Por lo tanto, el objetivo que estamos maximizando es simplemente la varianza muestral de los $N$ valores de $z_{i1}$. Nos referimos a $z_{11}, \\dots, z_{N1}$ como las puntuaciones del primer componente principal. \n",
    "\n",
    "Para resumir el procedimiento, al realizar el PCA, el primer componente principal de un conjunto de $M$ variables es la variable derivada formada como una combinación lineal de las variables originales que explica la mayor varianza. El segundo componente principal explica la mayor varianza de lo que queda una vez que se elimina el efecto del primer componente, y podemos proceder a través de $M$ iteraciones hasta que se explique toda la varianza. El PCA se utiliza más comúnmente cuando muchas de las variables están altamente correlacionadas entre sí y es deseable reducir su número a un conjunto independiente. El primer componente principal se puede definir de manera equivalente como una dirección que maximiza la varianza de los datos proyectados. El $i$-ésimo componente principal se puede tomar como una dirección ortogonal a los primeros $i-1$ componentes principales que maximizan la varianza de los datos proyectados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receta usando álgebra lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cualquiera de los objetivos, se puede demostrar que los componentes principales son vectores propios de la matriz de covarianza de los datos. Por lo tanto, los componentes principales a menudo se calculan mediante la descomposición propia de la matriz de covarianza de los datos o la descomposición en valores singulares de la matriz de datos que es lo que explicaremos a continuación.\n",
    "\n",
    "Supngamos que contamos con un conjunto de características $X_1, X_2, \\dots, X_M$ sobre una muestra de $N$ observaciones. La matriz de datos muestrales estará dada por:\n",
    "\n",
    "$$\n",
    "\\textbf{X} =\n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1M} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{NM} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación de los componentes principales**\n",
    "\n",
    "Hay una buena interpretación geométrica del primer componente principal. El vector de carga (loading) $\\varphi_{1}$ con elementos $\\varphi_{11}, \\varphi_{21}, \\dots, \\varphi_{M1}$ define una dirección en el espacio de características a lo largo de la cual los datos varían más. Si proyectamos los $N$ puntos de datos $x_1, \\dots, x_N$ en esta dirección, los valores proyectados son las puntuaciones del componente principal $z_{11}, \\dots, z_{N1}$ en sí.\n",
    "Después de que se haya determinado el primer componente principal $Z_1$ de las características, podemos encontrar el segundo componente principal $Z_2$. El segundo componente principal es la combinación lineal de $X_1, \\dots, X_M$ que tiene la varianza máxima de todas las combinaciones lineales que no están correlacionadas con $Z_1$. Las puntuaciones del segundo componente principal $z_{12}, z_{22}, \\dots, z_{N2}$ toman la forma\n",
    "\n",
    "$$\n",
    "z_{i2} = \\varphi_{12}x_{i1} + \\varphi_{22}x_{i2} + \\dots + \\varphi_{M2}x_{iM}\n",
    "$$\n",
    "\n",
    "donde $ \\varphi_{2}$ es el vector de carga del segundo componente principal, con elementos $ \\varphi_{12}, \\varphi_{22}, \\dots, \\varphi_{M2}$. Resulta que restringir que $Z_2$ no esté correlacionado con $Z_1$ es equivalente a restringir que la dirección $ \\varphi_2$ sea ortogonal (perpendicular) a la dirección $ \\varphi_{1}$.\n",
    "\n",
    "Entonces, resumiendo, podemos describir los vectores de carga de los componentes principales como las direcciones en el espacio de características a lo largo de las cuales los datos varían más, y los puntajes de los componentes principales como proyecciones a lo largo de estas direcciones. Sin embargo, también puede ser útil una interpretación alternativa de los componentes principales: los componentes principales proporcionan superficies lineales de baja dimensión que son las más cercanas a las observaciones. Aquí ampliamos esa interpretación.\n",
    "\n",
    "El primer vector de carga de los componentes principales tiene una propiedad muy especial: es la línea en el espacio $M$-dimensional que está más cerca de las $N$ observaciones (usando la distancia euclidiana al cuadrado promedio como medida de cercanía). El atractivo de esta interpretación es claro: buscamos una única dimensión de los datos que se encuentre lo más cerca posible de todos los puntos de datos, ya que dicha línea probablemente proporcionará un buen resumen de los datos.\n",
    "\n",
    "La noción de los componentes principales como las dimensiones que están más cerca de las $N$ observaciones se extiende más allá del primer componente principal. Por ejemplo, los dos primeros componentes principales de un conjunto de datos abarcan el plano más cercano a las $N$ observaciones, en términos de distancia euclidiana al cuadrado promedio. Los tres primeros componentes principales de un conjunto de datos abarcan el hiperplano tridimensional más cercano a las $N$ observaciones, y así sucesivamente.\n",
    "\n",
    "Usando esta interpretación, juntos los primeros $P$ vectores de puntuación de componentes principales y los primeros $P$ vectores de carga de componentes principales proporcionan la mejor aproximación $P$-dimensional (en términos de distancia euclidiana) a la i-ésima observación $x_{ij}$ . Esta representación se puede escribir como\n",
    "\n",
    "$$\n",
    "x_{ij} \\approx \\sum_{k=1}^P z_{ik}\\varphi_{jk}\n",
    "$$\n",
    "\n",
    "Podemos expresar esto de manera más formal escribiendo un problema de optimización. Supongamos que la matriz de datos $\\mathbf{X}$ está centrada en columnas. De todas las aproximaciones de la forma $x_{ij} \\approx \\sum_{k=1}^P a_{ik}b_{jk}$, podríamos preguntar cuál tiene la suma de cuadrados residual más pequeña:\n",
    "\n",
    "$$\n",
    "\\min_{\\textbf{A} \\in \\mathbb{R}^{N \\times P}, \\, \\textbf{B} \\in \\mathbb{R}^{M \\times P}} \\left[ \\sum_{j=1}^M \\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^P a_{ik}b_{jk} \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Aquí, $\\textbf{A}$ es una matriz $N \\times P$ cuyo elemento $(i, k)$ es $a_{ik}$, y $\\textbf{B}$ es un elemento $M \\times P$ cuyo elemento $(j, k)$ es $b_{jk}$.\n",
    "Se puede demostrar que para cualquier valor de $P$, las columnas de las matrices $\\hat{\\textbf{A}}$ y $\\hat{\\textbf{B}}$ que resuelven la ecuación anterior son de hecho los primeros $P$ vectores de puntaje y carga de componentes principales. En otras palabras, si $\\hat{\\mathbf{A}}$ y $\\hat{\\mathbf{B}}$ resuelven la ecuación anterior, entonces $\\hat{a}_{ik} = z_{ik}$ y $\\hat{b}_{jk} = \\varphi_{jk}$. Esto significa que el valor más pequeño posible del objetivo en la ecuación anterior es\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^M \\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^P z_{ik}\\varphi_{jk} \\right)^2\n",
    "$$\n",
    "\n",
    "En resumen, juntos, los vectores de puntaje de componentes principales $P$ y los vectores de carga de componentes principales $P$ pueden dar una buena aproximación a los datos cuando $P$ es suficientemente grande. Cuando $P = \\min(N − 1, M)$, entonces la representación es exacta: $x_{ij} = \\sum_{k=1}^P z_{ik}\\varphi_{jk}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La proporción de varianza explicada**\n",
    "\n",
    "Ahora podemos plantearnos una pregunta natural: ¿cuánta información de un conjunto de datos determinado se pierde al proyectar las observaciones sobre los primeros componentes principales? Es decir, ¿cuánta de la varianza de los datos no está contenida en los primeros componentes principales? En términos más generales, nos interesa saber la proporción de varianza explicada (PVE) por cada componente principal. La varianza total presente en un conjunto de datos (suponiendo que las variables se han centrado para tener media cero) se define como\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^M \\text{Var}(X_j) = \\sum_{j=1}^M\\frac{1}{N}\\sum_{i=1}^Nx_{ij}^2\n",
    "$$\n",
    "\n",
    "y la varianza explicada por el $k$-ésimo componente principal es\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N z_{ik}^2 = \\frac{1}{N}\\sum_{i=1}^N\\left( \\sum_{j=1}^M \\right)^2\n",
    "$$\n",
    "\n",
    "Por lo tanto, la PVE del $k$-ésimo componente principal viene dada por\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^N z_{ik}^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2} = \\frac{\\sum_{i=1}^N\\left( \\sum_{j=1}^M \\right)^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2}\n",
    "$$\n",
    "\n",
    "El PVE de cada componente principal es una cantidad positiva. Para calcular el PVE acumulado de los primeros $P$ componentes principales, podemos simplemente sumar cada uno de los primeros $P$ PVE. En total, hay $\\min(N − 1, M)$ componentes principales, y sus PVE suman uno. Se puede demostrar que los primeros $P$ vectores de carga y puntuación del componente principal se pueden interpretar como la mejor aproximación $P$ dimensional a los datos, en términos de suma de cuadrados residuales. Resulta que la varianza de los datos se puede descomponer en la varianza de los primeros $M$ componentes principales más el error cuadrático medio de esta aproximación $P$-dimensional, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\sum_{j=1}^M\\frac{1}{N}\\sum_{i=1}^Nx_{ij}^2}_{\\text{Var. de los datos}} = \\underbrace{\\sum_{k=1}^P\\frac{1}{N}\\sum_{i=1}^Nz_{ik}^2}_{\\text{Var. de los primeros P componentes principales}} + \\underbrace{\\frac{1}{N}\\sum_{j=1}^M\\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^M z_{ik}\\varphi_{jk} \\right)^2}_{\\text{MSE de la aproximación de dimensión M}}\n",
    "$$\n",
    "\n",
    "Dado que el primer término es fijo, vemos que al maximizar la varianza de los primeros $P$ componentes principales, minimizamos el error cuadrático medio de la aproximación de dimensión $P$, y viceversa. Esto explica por qué los componentes principales pueden considerarse equivalentemente como la minimización del error de aproximación o la maximización de la varianza).\n",
    "\n",
    "Además, podemos usar el resultado anterior para ver que el PVE definido en la ecuación anterior a la anterior es igual a\n",
    "\n",
    "$$\n",
    "1 - \\frac{\\sum_{j=1}^M\\sum_{i=1}^N \\left(x_{ij} - \\sum_{k=1}^M z_{ik}\\varphi_{jk} \\right)^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "$$\n",
    "\n",
    "donde TSS representa la suma total de elementos al cuadrado de $\\mathbf{X}$, y RSS representa la suma residual de cuadrados de la aproximación $P$-dimensional dada por los componentes principales. Recordando la definición de $R^2$, esto significa que podemos interpretar el PVE como el $R^2$ de la aproximación para $X$ dada por los primeros $P$ componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decidir cuántos componentes principales utilizar**\n",
    "\n",
    "En general, una matriz de datos $N \\times M$ $\\mathbf{X}$ tiene $\\min(N − 1, M)$ componentes principales distintos. Sin embargo, por lo general no nos interesan todos ellos; más bien, nos gustaría utilizar solo los primeros componentes principales para visualizar o interpretar los datos. De hecho, nos gustaría utilizar la menor cantidad de componentes principales necesarios para comprender bien los datos. ¿Cuántos componentes principales se necesitan? Desafortunadamente, no hay una respuesta única (¡o simple!) a esta pregunta.\n",
    "\n",
    "Por lo general, decidimos la cantidad de componentes principales necesarios para visualizar los datos examinando un gráfico de sedimentación. Elegimos el menor número de componentes principales necesarios para explicar una cantidad considerable de la variación en los datos. Esto se hace observando el gráfico de sedimentación y buscando un punto en el que la proporción de varianza explicada por cada componente principal posterior disminuye. Esta disminución se suele denominar codo en el gráfico de sedimentación.\n",
    "\n",
    "Sin embargo, este tipo de análisis visual es inherentemente ad hoc. Desafortunadamente, no existe una forma objetiva y aceptada de decidir cuántos componentes principales son suficientes. De hecho, la cuestión de cuántos componentes principales son suficientes está inherentemente mal definida y dependerá del área de aplicación específica y del conjunto de datos específico. En la práctica, tendemos a observar los primeros componentes principales para encontrar patrones interesantes en los datos. Si no se encuentran patrones interesantes en los primeros componentes principales, es poco probable que otros componentes principales sean de interés. Por el contrario, si los primeros componentes principales son interesantes, normalmente continuamos observando los componentes principales posteriores hasta que no se encuentren más patrones interesantes. Se reconoce que este es un enfoque subjetivo y refleja el hecho de que el PCA se utiliza generalmente como una herramienta para el análisis exploratorio de datos.\n",
    "\n",
    "Por otro lado, si calculamos los componentes principales para su uso en un análisis supervisado, como la regresión de componentes principales, existe una forma simple y objetiva de determinar cuántos componentes principales utilizar: podemos tratar la cantidad de vectores de puntaje de componentes principales que se utilizarán en la regresión como un parámetro de ajuste que se seleccionará mediante validación cruzada o un enfoque relacionado. La simplicidad comparativa de seleccionar la cantidad de componentes principales para un análisis supervisado es una manifestación del hecho de que los análisis supervisados ​​tienden a definirse con mayor claridad y evaluarse de manera más objetiva que los análisis no supervisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVVglnoM60iuQb8Y59tHbX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
