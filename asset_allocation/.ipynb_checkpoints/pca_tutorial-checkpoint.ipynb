{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b30afd6-8743-495c-807f-003e9446e634",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d13d20-a52a-4f5e-8398-c4a667d614ee",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a widely used statistical technique for dimensionality reduction in datasets. This method identifies the directions in which the data exhibit the greatest variability and transforms the data into a new coordinate system based on these directions. PCA was developed by Karl Pearson in 1901 and later refined by Harold Hotelling in the 1930s.\n",
    "\n",
    "PCA is an unsupervised method, meaning it relies solely on a set of features $X_1, X_2, \\dots, X_M$ without considering any associated response variable $Y$. Although its primary use is for dimensionality reduction, it is also applied in supervised learning problems, for example, by using the principal components as predictors in machine learning algorithms instead of the larger original set of variables. Additionally, PCA is a powerful tool for data visualization.\n",
    "\n",
    "PCA transforms the data into a new coordinate system where the principal components represent the greatest variability in the data. These components are sequences of unit vectors that optimally fit the data, minimizing the average squared perpendicular distance between the points and the line. The directions of the principal components correspond to those in the feature space along which the data show the greatest variability, defining lines and subspaces that closely approximate the data cloud.\n",
    "\n",
    "One of the main advantages of PCA is its ability to summarize a large set of correlated variables into a smaller number of representative variables that, together, explain most of the variability of the original set. This is especially useful in contexts with a large number of variables, where visualizing the data becomes a challenge. For example, if you have $N$ observations and a set of $M$ features $X_1, X_2, \\dots, X_M$, you could generate $\\binom{M}{2} = \\frac{M(M - 1)}{2}$ scatterplots to visualize the data. This means that for $M = 10$, there would be 45 possible scatterplots, which can become impractical if $M$ is large. Moreover, it's likely that none of these plots alone would be informative, as each captures only a fraction of the total information. Clearly, a more efficient method is needed to visualize the $N$ observations when $M$ is large, and this is where PCA is particularly useful.\n",
    "\n",
    "PCA allows for finding a low-dimensional representation of the data that captures as much variation as possible. While each of the $N$ observations resides in an $M$-dimensional space, not all of these dimensions are equally relevant. PCA identifies a small number of dimensions that are the most interesting, where the degree of interest is measured by the amount of variation in the observations along each dimension. Each of these dimensions is a linear combination of the original $M$ features.\n",
    "\n",
    "Therefore, PCA provides an effective solution for dimensionality reduction and data visualization in a low-dimensional space. For example, if two principal components can be derived that capture most of the variability in the data, the observations can be represented in a two-dimensional scatterplot, making it easier to identify patterns and clusters of related data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f6bd7-5841-4738-9b6d-d6a530b7cc97",
   "metadata": {},
   "source": [
    "## PCA Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb63fcf-d838-4636-ba89-d8ee8756f1df",
   "metadata": {},
   "source": [
    "In what follows, we explain the procedure for finding the principal components. Before diving into the details, it's worth noting that in PCA, the variables should be centered so that they have a mean of zero. Additionally, the results obtained when performing PCA also depend on whether the variables have been individually scaled (i.e., each multiplied by a different constant).\n",
    "\n",
    "What we seek is a mapping that transforms the original features $X_1, X_2, \\dots, X_M$ into new features $Z_1, Z_2, \\dots, Z_M$ in such a way that the variance is maximized, subject to the constraint that the column vectors forming the matrix representation of this transformation have unit norm. Mathematically, this means that the first principal component of the transformed feature set is the normalized linear combination of the features:\n",
    "\n",
    "$$\n",
    "Z_1 = \\gamma_{11}X_1 + \\gamma_{21}X_2 + \\dots + \\gamma_{M1}X_M = \\sum_{j=1} \\gamma_{j1}X_j = \\boldsymbol{\\gamma}_1^T \\mathbf{X} = \n",
    "\\left( \\gamma_{11}, \\gamma_{21}, \\dots, \\gamma_{M1} \\right)\n",
    "\\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "\\vdots \\\\\n",
    "X_M\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We refer to the elements $\\gamma_{11}, \\gamma_{21}, \\dots, \\gamma_{M1}$ as the loadings of the first principal component; collectively, the loadings form the principal component loading vector, $\\boldsymbol{\\gamma}_1 = (\\gamma_{11}, \\gamma_{21}, \\dots, \\gamma_{M1})^T$. We constrain the loadings so that their sum of squares equals one, as otherwise, making these elements arbitrarily large in absolute value could result in an arbitrarily large variance. Therefore, to obtain the first principal component, we need to find the linear combination of the feature values that has the highest sample variance subject to the constraint $\\lVert \\gamma_1 \\rVert^2 = \\sum_{j=1}^M \\gamma_{j1}^2 = 1$. In other words, the first principal component loading vector solves the following constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\gamma_{11}, \\dots, \\gamma_{M1}} \\left\\{ \\text{Var}\\left[ Z_1 \\right] \\right\\}, \\,\\,\\, \\text{subject to} \\,\\,\\, \\lVert \\boldsymbol{\\gamma_1} \\rVert^2 = 1\n",
    "$$\n",
    "\n",
    "For convenience, we write the variance of $Z_1$ in matrix form, which is the objective function:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left[ Z_1 \\right] = \\text{Var}\\left[ \\boldsymbol{\\gamma}_1^T \\mathbf{X} \\right] = \\boldsymbol{\\gamma}_1^T \\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_1\n",
    "$$\n",
    "\n",
    "Thus, the problem takes the form:\n",
    "\n",
    "$$\n",
    "\\max_{\\gamma_{11}, \\dots, \\gamma_{M1}} \\left\\{ \\boldsymbol{\\gamma}_1^T \\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_1 \\right\\}, \\,\\,\\, \\text{subject to} \\,\\,\\, \\boldsymbol{\\gamma}_1^T \\boldsymbol{\\gamma}_1 = 1\n",
    "$$\n",
    "\n",
    "A well-known method for solving constrained optimization problems is the method of Lagrange multipliers. For our problem, the Lagrangian is defined as the objective function minus a constant for each of the constraints:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\gamma}_1) = \\boldsymbol{\\gamma}_1^T\\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_1 - \\lambda_1\\left( \\boldsymbol{\\gamma}_1^T\\boldsymbol{\\gamma}_1 - 1 \\right)\n",
    "$$\n",
    "\n",
    "To maximize the Lagrangian, we must find its critical points:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}_1} \n",
    "& = \\Sigma\\boldsymbol{\\gamma}_1 + \\boldsymbol{\\gamma}_1^T\\Sigma - \\lambda_1\\boldsymbol{\\gamma}_1 - \\lambda_1\\boldsymbol{\\gamma}_1^T \\\\\n",
    "& = \\Sigma\\boldsymbol{\\gamma}_1 + \\Sigma\\boldsymbol{\\gamma}_1 - \\lambda_1\\mathbb{1}\\boldsymbol{\\gamma}_1 - \\lambda_1\\mathbb{1}\\boldsymbol{\\gamma}_1^T \\\\\n",
    "& = 2\\Sigma\\boldsymbol{\\gamma}_1 - 2\\lambda_1\\mathbb{1}\\boldsymbol{\\gamma}_1 \\\\\n",
    "& = 2\\left(\\Sigma- \\lambda_1\\mathbb{1}\\right)\\boldsymbol{\\gamma}_1  \\\\\n",
    "& = \\boldsymbol{0} \\\\ \\\\\n",
    "& \\Rightarrow \\left(\\Sigma - \\lambda_1\\mathbb{1}\\right)\\boldsymbol{\\gamma}_1 = \\boldsymbol{0}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "By the Rouché–Frobenius theorem, for a homogeneous system of equations to have non-trivial solutions, the associated matrix must be singular, which means its determinant is zero: $\\text{det}\\left(\\Sigma - \\lambda_1\\mathbb{1}\\right) = 0$. This implies that $\\lambda_1$ is an eigenvalue of the covariance matrix $\\Sigma$. Additionally, we know that since the covariance matrix $\\Sigma$ is $M \\times M$ and positive definite, it has exactly $M$ eigenvalues $\\lambda_i, \\, i = 1, 2, \\dots, M$ that satisfy $\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_M$. Therefore, since the variance of the first principal component $Z_1$ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left[ Z_1 \\right] = \\text{Var}\\left[ \\boldsymbol{\\gamma}_1^T \\mathbf{X} \\right] = \\boldsymbol{\\gamma}_1^T \\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_1 = \\boldsymbol{\\gamma}_1^T \\lambda_1\\mathbb{1} \\boldsymbol{\\gamma}_1 = \\lambda_1\\boldsymbol{\\gamma}_1^T\\boldsymbol{\\gamma}_1 = \\lambda_1\\lVert \\boldsymbol{\\gamma_1} \\rVert^2 = \\lambda_1\n",
    "$$\n",
    "\n",
    "this means that the maximum variance of the first principal component corresponds to the largest eigenvalue of the covariance matrix $\\lambda_1$, and the transformation vector $\\boldsymbol{\\gamma}_1$ corresponds to the eigenvector of the covariance matrix associated with this eigenvalue $\\lambda_1$.\n",
    "\n",
    "The second principal component is the linear combination of $X_1, \\dots, X_M$ that has the maximum variance among all linear combinations that are uncorrelated with $Z_1$:\n",
    "\n",
    "$$\n",
    "Z_2 = \\gamma_{12}X_1 + \\gamma_{22}X_2 + \\dots + \\gamma_{M2}X_M = \\sum_{j=1} \\gamma_{j2}X_j = \\boldsymbol{\\gamma}_2^T \\mathbf{X} = \n",
    "\\left( \\gamma_{12}, \\gamma_{22}, \\dots, \\gamma_{M2} \\right)\n",
    "\\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "\\vdots \\\\\n",
    "X_M\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\gamma}_2$ is the loading vector for the second principal component, with elements $\\gamma_{12}, \\gamma_{22}, \\dots, \\gamma_{M2}$. Therefore, to find the second principal component, we repeat the process used to find the first component but add the constraint that the first and second principal components are uncorrelated, which is equivalent to saying that $\\text{Cov}(Z_1, Z_2) = 0$. Since:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Cov}(Z_1, Z_2) \n",
    "& = \\text{Cov}(\\boldsymbol{\\gamma}_2^T\\mathbf{X}, \\boldsymbol{\\gamma}_1^T\\mathbf{X}) \\\\\n",
    "& = \\text{Cov}(\\boldsymbol{\\gamma}_2^T\\mathbf{X}, \\mathbf{X}^T\\boldsymbol{\\gamma}_1) \\\\\n",
    "& = \\boldsymbol{\\gamma}_2^T\\text{Cov}(\\mathbf{X}, \\mathbf{X}^T)\\boldsymbol{\\gamma}_1 \\\\\n",
    "& = \\boldsymbol{\\gamma}_2^T\\mathbb{E}\\left[ \\left(\\mathbf{X} - \\boldsymbol{\\mu}_X \\right)\\left(\\mathbf{X} - \\boldsymbol{\\mu}_X \\right)^T \\right]\\boldsymbol{\\gamma}_1, \\,\\,\\, \\text{(using properties of the transpose)} \\\\\n",
    "& = \\boldsymbol{\\gamma}_2^T \\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_1 \\\\\n",
    "& = \\boldsymbol{\\gamma}_2^T \\lambda_1\\mathbb{1} \\boldsymbol{\\gamma}_1 \\\\\n",
    "& \\Rightarrow \\boldsymbol{\\gamma}_2^T \\boldsymbol{\\gamma}_1 = 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "It follows that constraining $Z_2$ to be uncorrelated with $Z_1$ is equivalent to constraining the direction $\\boldsymbol{\\gamma}_2$ to be orthogonal (perpendicular) to the direction $\\boldsymbol{\\gamma}_{1}$. Therefore, finding the second principal component reduces to solving a constrained optimization problem with two conditions:\n",
    "\n",
    "$$\n",
    "\\max_{\\gamma_{12}, \\dots, \\gamma_{M2}} \\left\\{ \\text{Var}\\left[ Z_2 \\right] \\right\\}, \\,\\,\\, \\text{sujeto a} \\,\\,\\, \\lVert \\boldsymbol{\\gamma_2} \\rVert^2 = 1, \\,\\,\\, \\text{y} \\,\\,\\, \\boldsymbol{\\gamma}_2^T\\boldsymbol{\\gamma}_1 = 0\n",
    "$$\n",
    "\n",
    "The Lagrangian for this case is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\gamma}_2) = \\boldsymbol{\\gamma}_2^T\\boldsymbol{\\Sigma} \\boldsymbol{\\gamma}_2 - \\lambda_2\\left( \\boldsymbol{\\gamma}_2^T\\boldsymbol{\\gamma}_2- 1 \\right) - \\alpha\\left( \\boldsymbol{\\gamma}_2^T \\boldsymbol{\\gamma}_1 \\right)\n",
    "$$\n",
    "\n",
    "We seek the critical points of the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\gamma}_2} \n",
    "& = 2\\Sigma\\boldsymbol{\\gamma}_2 - 2\\lambda_2\\boldsymbol{\\gamma}_2 - \\alpha\\boldsymbol{\\gamma}_1 \\\\\n",
    "& = 2\\boldsymbol{\\gamma}_1^T\\Sigma\\boldsymbol{\\gamma}_2 - 2\\lambda_2\\boldsymbol{\\gamma}_1^T\\boldsymbol{\\gamma}_2 - \\alpha\\boldsymbol{\\gamma}_1^T\\boldsymbol{\\gamma}_1 \\\\\n",
    "& = 2\\boldsymbol{\\gamma}_1^T\\Sigma\\boldsymbol{\\gamma}_2 - 2\\lambda_2\\left(\\boldsymbol{\\gamma}_2^T\\boldsymbol{\\gamma}_1\\right)^T - \\alpha\\lVert \\boldsymbol{\\gamma_1} \\rVert^2 \\\\\n",
    "& = 2\\boldsymbol{\\gamma}_1^T\\lambda_2\\boldsymbol{\\gamma}_2 - \\alpha \\mathbb{1} \\,\\,\\, \\text{(using the eigenvector equation of} \\, \\boldsymbol{\\gamma}_1 \\text{)}\\\\\n",
    "& = - \\alpha \\mathbb{1} \\\\\n",
    "& = \\boldsymbol{0} \\\\ \\\\\n",
    "& \\Rightarrow \\alpha = 0 \\\\ \\\\\n",
    "& \\Rightarrow \\left(\\Sigma - \\lambda_2\\mathbb{1}\\right)\\boldsymbol{\\gamma}_2 = \\boldsymbol{0}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "In this way we obtain another eigenvalue equation and the same strategy of choosing $\\boldsymbol{\\gamma}_2$ as the eigenvector associated with the second largest eigenvalue yields the second principal component.\n",
    "\n",
    "To summarize the procedure, when performing PCA, the first principal component of a set of $M$ variables is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance of what remains once the effect of the first component is removed, and we can proceed through $M$ iterations until all the variance is explained. The first principal component can be equivalently defined as a direction that maximizes the variance of the projected data and the $j$-th principal component can be taken as a direction orthogonal to the first $j-1$ principal components that maximize the variance of the projected data.\n",
    "\n",
    "Finally, we can construct a matrix from the $\\boldsymbol{\\gamma}_j$ eigenvectors, namely:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Gamma} = \\left( \\boldsymbol{\\gamma}_1, \\boldsymbol{\\gamma}_2, \\dots, \\boldsymbol{\\gamma}_M \\right) = \n",
    "\\begin{bmatrix}\n",
    "\\gamma_{11} & \\gamma_{12} & \\cdots & \\gamma_{1M} \\\\\n",
    "\\gamma_{21} & \\gamma_{22} & \\cdots & \\gamma_{2M} \\\\\n",
    "\\vdots      & \\cdots      & \\ddots & \\vdots      \\\\\n",
    "\\gamma_{M1} & \\gamma_{M2} & \\cdots & \\gamma_{MM} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and since the $\\boldsymbol{\\gamma}_j$ are orthonormal to each other, the matrix $\\boldsymbol{\\Gamma}$ will be an orthogonal matrix, i.e. $\\boldsymbol{\\Gamma}^T\\boldsymbol{\\Gamma} = \\mathbb{1}$ which can be understood as a rotation in the feature space, hence PCA reduces to finding a rotation of the coordinate system that maximizes the variance:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\boldsymbol{\\Gamma} \\mathbf{X}\n",
    "$$\n",
    "\n",
    "The matrix $\\boldsymbol{\\Gamma}$ is precisely the matrix that allows to diagonalize the covariance matrix $\\Sigma$ and whose diagonal elements will be the variances of the new features $\\mathbf{Z}$. Mathematically, this can be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\Lambda = \\text{Var}\\left[ \\mathbf{Z} \\right] = \\text{Var}\\left[ \\boldsymbol{\\Gamma} \\mathbf{X} \\right] = \\boldsymbol{\\Gamma}^T\\Sigma\\boldsymbol{\\Gamma} \\Rightarrow \\Sigma = \\boldsymbol{\\Gamma}\\Lambda \\boldsymbol{\\Gamma}^T\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ takes the form:\n",
    "\n",
    "$$\n",
    "\\Lambda =\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0         & \\cdots & 0         \\\\\n",
    "0         & \\lambda_2 & \\cdots & 0         \\\\\n",
    "\\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "0         & 0         & \\cdots & \\lambda_M \\\\\n",
    "\\end{bmatrix}\n",
    ", \\,\\,\\, \\text{donde} \\,\\,\\, \\lambda_i = \\text{Var}\\left[ Z_i \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5c306-1f03-4d37-ae34-9c9dec3a21b4",
   "metadata": {},
   "source": [
    "## The proportion of variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5135826-fa0f-431e-9b12-80db06426cd8",
   "metadata": {},
   "source": [
    "When projecting data onto the first principal components, a natural question arises: how much information is lost in this process? In other words, how much of the data's variance is not explained by the first principal components? The proportion of variance explained (PVE) by each principal component gives us a clue about the amount of information retained.\n",
    "\n",
    "The PVE is calculated as the variance explained by each principal component divided by the total variance of the data:\n",
    "\n",
    "$$\n",
    "\\text{PVE}_j = \\frac{\\text{Var}(Z_j)}{\\sum_{j=1}^M \\text{Var}(X_j)} = \\frac{\\lambda_j}{\\text{Tr}\\left( \\Sigma \\right)} = \\frac{\\lambda_j}{\\text{Tr}\\left( \\boldsymbol{\\Gamma}\\Lambda \\boldsymbol{\\Gamma}^T \\right)} = \\frac{\\lambda_j}{\\text{Tr}\\left( \\boldsymbol{\\Gamma}^T\\boldsymbol{\\Gamma}\\Lambda \\right)} = \\frac{\\lambda_j}{\\text{Tr}\\left( \\mathbb{1}\\Lambda \\right)} = \\frac{\\lambda_j}{\\sum_{j=1}^M\\lambda_j} > 0\n",
    "$$\n",
    "\n",
    "By summing the PVE of the first $q$ principal components, we obtain the cumulative PVE, which indicates the amount of variance explained by these components:\n",
    "\n",
    "$$\n",
    "\\text{PVE} = \\frac{\\sum_{j=1}^q\\lambda_j}{\\sum_{j=1}^M\\lambda_j}\n",
    "$$\n",
    "\n",
    "But how many principal components should we use? The answer is not unique and depends on the dataset and the application area. A common approach is to use a scree plot to determine the number of principal components needed to explain a significant amount of the variation in the data. We look for a point where the proportion of variance explained by each principal component decreases, known as the elbow in the scree plot.\n",
    "\n",
    "However, this approach is subjective, and there is no objective way to decide how many principal components are sufficient. In practice, the first principal components are examined to find interesting patterns in the data, and this process continues until no more interesting patterns are found. PCA is generally used as a tool for exploratory data analysis, and this approach reflects its subjective nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a0a44-b87c-4e9d-9ea2-c7083b5ffd2f",
   "metadata": {},
   "source": [
    "## `Python` Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdecf47-0a47-4ce0-a277-944c68d80ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components (Z):\n",
      "==========================\n",
      "\n",
      "[[ 0.68418751  0.79610645  1.75836368]\n",
      " [ 0.77683835  0.6367522  -1.15535049]\n",
      " [ 1.56100741  1.13398589  1.85919827]\n",
      " [-0.62368171  4.32887472 -0.25106172]\n",
      " [-0.00736948 -0.86272086  0.08519462]\n",
      " [-0.11280011 -1.08654375  0.52541782]\n",
      " [-2.21857906 -1.14428366  1.68106603]\n",
      " [ 0.1110877   1.51678127  2.35364845]\n",
      " [-0.57139453  0.76002065 -0.17354227]\n",
      " [-1.20233214  0.56645817 -0.26560399]\n",
      " [ 0.51465461 -0.05546718 -0.32757921]\n",
      " [ 0.72993531 -0.51183776 -1.15255471]\n",
      " [-1.47115584  0.59378641 -0.05628557]\n",
      " [ 1.24348714 -0.36212297  0.56740429]\n",
      " [-1.90428336  0.64081588 -1.0829176 ]\n",
      " [-0.99891706  0.79758953  0.81671352]\n",
      " [-1.09768857 -0.50157898 -0.46461926]\n",
      " [-0.36573984 -1.43155793 -0.63581788]\n",
      " [ 0.47290273 -0.79913458 -1.40646482]\n",
      " [ 1.51872242 -0.34687035  0.76358111]\n",
      " [-0.13924701 -1.78827459  1.01305827]\n",
      " [-0.99971559  0.01945489  0.4473964 ]\n",
      " [-0.82558531  0.95361112  0.58415806]\n",
      " [ 0.96112459  2.9359276   0.06712562]\n",
      " [ 0.83332458  1.33521214 -0.97099467]\n",
      " [ 1.09108796 -0.00944479 -0.1381175 ]\n",
      " [ 0.06117775  0.59296897  0.18334568]\n",
      " [ 0.96670789  0.63858337 -0.09146953]\n",
      " [-1.96815103 -0.56146401 -0.66539071]\n",
      " [ 1.17547539  0.53172851  0.45752895]\n",
      " [-1.80930863 -0.27277853  0.8835575 ]\n",
      " [ 0.78646374 -0.18426196 -2.3571458 ]\n",
      " [ 0.51476427 -0.33716932  1.33492453]\n",
      " [ 0.33825198 -1.74458098 -0.09224614]\n",
      " [ 2.37469046 -1.08775439  1.59414166]\n",
      " [ 1.5465679  -1.13510986 -1.15883539]\n",
      " [-0.93558502  1.79971961  0.81123689]\n",
      " [-1.23406544  0.14875767 -1.56213959]\n",
      " [ 1.5719935   0.76265517 -0.55260882]\n",
      " [-0.45355962  1.62827967  0.66835469]\n",
      " [ 2.68163437 -1.23082655 -0.39011619]\n",
      " [ 2.6351259   1.31818042 -0.22134637]\n",
      " [-0.26691435  0.58488217  0.22554372]\n",
      " [ 0.70372644 -0.21844294 -0.52537334]\n",
      " [-0.09042135  0.24853986 -0.44644408]\n",
      " [ 0.07445528  0.42372802  1.32999281]\n",
      " [-0.84222262  0.14460118  0.91182773]\n",
      " [ 1.34391621 -0.04204405 -0.23065497]\n",
      " [-1.74424983 -1.47128287 -0.43911679]\n",
      " [-1.19432918 -0.62242257 -0.13696279]\n",
      " [-0.67528316 -1.33750613  0.85474846]\n",
      " [-1.4777277   0.09134691 -0.26485695]\n",
      " [-1.09568444  1.79893993 -1.23255442]\n",
      " [ 0.16539708 -0.04159977  0.81031287]\n",
      " [ 1.90443856  0.30512454  0.39754788]\n",
      " [ 0.94024111  0.9626186   0.76003484]\n",
      " [ 0.49024609 -1.11385989  0.38722212]\n",
      " [-0.27273922 -1.71651285  1.11628081]\n",
      " [ 0.00694407  0.50973771 -0.57163527]\n",
      " [ 0.1469742  -1.21077331 -0.78033985]\n",
      " [ 0.24991831 -0.48606636 -0.3420246 ]\n",
      " [ 0.82008314 -1.03948369  1.08002399]\n",
      " [ 0.04699771 -0.68378357  0.74951201]\n",
      " [-0.52282004  1.09876774 -1.58262391]\n",
      " [-3.84705368  0.04343834 -0.17440654]\n",
      " [-1.6495078   1.4230306  -0.79826008]\n",
      " [-2.26475792  0.34715697 -1.51108154]\n",
      " [-1.44514554  1.18480746  1.2245534 ]\n",
      " [ 0.72017398 -0.31440574  1.10987535]\n",
      " [-1.09309799 -0.89373079  0.92935251]\n",
      " [-0.7315892  -1.38045471  0.51383774]\n",
      " [-1.2334966  -0.50559075  1.66548535]\n",
      " [ 0.92200198  0.52727117 -1.40029434]\n",
      " [ 1.46919383  0.59122372 -0.56969832]\n",
      " [ 0.1029958   0.78488386 -1.76393034]\n",
      " [ 0.24969126 -0.73796195 -2.0169265 ]\n",
      " [-0.48617169 -0.54377623  0.05965364]\n",
      " [ 1.43284973  0.02094995  1.12790565]\n",
      " [ 0.55363827  0.03325416 -0.05620078]\n",
      " [ 1.95824753 -0.383672   -0.31145288]\n",
      " [-0.05127434 -1.0843576  -0.15589304]\n",
      " [ 0.98585089  0.33309083  0.31887896]\n",
      " [-1.11314365 -1.56865656  0.21693324]\n",
      " [-0.14323552 -0.58621953 -0.67386229]\n",
      " [-0.70184428  1.82186941  0.25462735]\n",
      " [ 1.36799687  1.07623446 -0.69150758]\n",
      " [-0.33889509 -0.76825465  0.85679787]\n",
      " [ 0.56428712 -0.25849046 -0.21361952]\n",
      " [ 0.00795663 -0.77426843  1.09993192]\n",
      " [-0.31422935  0.54427522 -0.40065264]\n",
      " [-0.30840064 -0.74413396 -1.5024764 ]\n",
      " [ 0.64771865 -0.71017282  0.75248916]\n",
      " [-0.64268849 -0.23845883 -0.15095582]\n",
      " [ 0.31652354  0.49516169 -1.62305462]\n",
      " [-0.50499778  0.87608148  1.77126038]\n",
      " [ 0.6122422  -1.87553227 -0.59173947]\n",
      " [-0.429309    0.79059635  0.93518094]\n",
      " [ 0.12540838 -0.33521275 -1.00420168]\n",
      " [ 1.42330349 -0.19354832 -1.23490358]\n",
      " [-1.08424198 -2.16340331 -1.34531359]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def PCA(X, q):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the input data matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The input data matrix of shape (N, M), where N is the number of samples \n",
    "                       and M is the number of features.\n",
    "    q (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A matrix Z of dimensions (N, q) containing the q principal components.\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data by subtracting the mean of each column (feature)\n",
    "    N, M = X.shape\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered = X - mean\n",
    "\n",
    "    # Step 2: Compute the covariance matrix\n",
    "    covariance_matrix = X_centered.T @ X_centered / N\n",
    "\n",
    "    # Step 3: Perform eigenvalue decomposition on the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Step 4: Sort the eigenvectors by decreasing eigenvalue magnitudes\n",
    "    sorted_indices = np.argsort(-eigenvalues)\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "\n",
    "    # Step 5: Select the first q eigenvectors\n",
    "    gamma_q = eigenvectors[:, :q]\n",
    "\n",
    "    # Step 6: Compute the principal components Z\n",
    "    Z = X_centered @ gamma_q\n",
    "\n",
    "    # Return the principal components matrix Z\n",
    "    return Z\n",
    "\n",
    "# ==============\n",
    "# Implementation\n",
    "# ==============\n",
    "\n",
    "# Example usage\n",
    "X = np.random.randn(100, 5)  # Generate random data (100 samples, 5 features)\n",
    "q = 3  # Number of principal components to return\n",
    "\n",
    "# Perform PCA and obtain the first q principal components\n",
    "Z = PCA(X, q)\n",
    "\n",
    "# Print the resulting matrix of principal components Z\n",
    "print(\"Principal Components (Z):\")\n",
    "print(\"==========================\\n\")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e646f-7c33-458e-b33e-6c1d241a0646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
