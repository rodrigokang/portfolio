{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3AYxvET2YT2"
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**James**\n",
    "\n",
    "Cuando nos enfrentamos a un gran conjunto de variables correlacionadas, los componentes principales nos permiten resumir este conjunto con un número menor de variables representativas que, en conjunto, explican la mayor parte de la variabilidad del conjunto original. Las direcciones de los componentes principales son las direcciones en el espacio de características a lo largo de las cuales los datos originales son altamente variables. Estas direcciones también definen líneas y subespacios que están lo más cerca posible de la nube de datos. Para realizar la regresión de componentes principales, simplemente usamos componentes principales como predictores en un modelo de regresión en lugar del conjunto original más grande de variables.\n",
    "\n",
    "El análisis de componentes principales (PCA) se refiere al proceso mediante el cual se calculan los componentes principales y el uso posterior de estos componentes para comprender los datos. El PCA es un enfoque no supervisado, ya que involucra solo un conjunto de características $X_1, X_2, \\dots, X_M$ y ninguna respuesta asociada Y. Además de producir variables derivadas para su uso en problemas de aprendizaje supervisado, el PCA también sirve como una herramienta para la visualización de datos (visualización de las observaciones o visualización de las variables). También se puede utilizar como herramienta para la imputación de datos, es decir, para completar valores faltantes en una matriz de datos.\n",
    "\n",
    "Supongamos que deseamos visualizar $N$ observaciones con mediciones en un conjunto de $M$ características, $X_1, X_2, \\dots, X_M$, como parte de un análisis exploratorio de datos. Podríamos hacerlo examinando scatterplots bidimensionales de los datos, cada uno de los cuales contiene las mediciones de las $N$ observaciones en dos de las características. Sin embargo, existen $\\binom{M}{2} = M(M - 1)/2$ scatterplots de este tipo; por ejemplo, con $M = 10$ ¡hay 45 diagramas! Si $M$ es grande, entonces ciertamente no será posible observarlos todos; además, lo más probable es que ninguno de ellos sea informativo ya que cada uno contiene solo una pequeña fracción de la información total presente en el conjunto de datos. Claramente, se requiere un mejor método para visualizar las $N$ observaciones cuando $M$ es grande. En particular, nos gustaría encontrar una representación de baja dimensión de los datos que capture la mayor cantidad de información posible. Por ejemplo, si podemos obtener una representación bidimensional de los datos que capture la mayor parte de la información, entonces podemos representar gráficamente las observaciones en este espacio de baja dimensión. El PCA proporciona una herramienta para hacer precisamente esto. Busca una representación de baja dimensión de un conjunto de datos que contenga la mayor cantidad posible de variación. La idea es que cada una de las $N$ observaciones viva en un espacio de $M$ dimensiones, pero no todas estas dimensiones son igualmente interesantes. El PCA busca un pequeño número de dimensiones que sean lo más interesantes posible, donde el concepto de interés se mide por la cantidad en que varían las observaciones a lo largo de cada dimensión. Cada una de las dimensiones encontradas por el PCA es una combinación lineal de las $M$ características. Ahora explicamos la manera en que se encuentran estas dimensiones, o componentes principales. El primer componente principal de un conjunto de características $X_1, X_2, \\dots, X_M$ es la combinación lineal normalizada de las características\n",
    "\n",
    "$$\n",
    "Z_1 = \\varphi_{11}X_1 + \\varphi_{21}X_2 + \\dots + \\varphi_{M1}X_M\n",
    "$$\n",
    "\n",
    "que tiene la varianza más grande. Por normalizado, queremos decir que $\\sum_{j=1}^M\\varphi_{j1}^2 = 1$. Nos referimos a los elementos $\\varphi_{11}, \\varphi_{21}, \\dots + \\varphi_{M1}$ como los loadings del primer componente principal; en conjunto, los loadings forman el vector de cargas del componente principal, $\\varphi_1 = (\\varphi_{11}, \\varphi_{21}, \\dots + \\varphi_{M1})^T$. Restringimos los loadings de modo que su suma de cuadrados sea igual a uno, ya que de lo contrario, establecer que estos elementos sean arbitrariamente grandes en valor absoluto podría resultar en una varianza arbitrariamente grande.\n",
    "\n",
    "Dado un conjunto de datos $N \\times M$ $\\textbf{X}$, ¿cómo calculamos el primer componente principal? Dado que solo nos interesa la varianza, suponemos que cada una de las variables en $\\textbf{X}$ se ha centrado para que tenga una media cero (es decir, las medias de las columnas de $\\textbf{X}$ son cero). Luego buscamos la combinación lineal de los valores de las características de la muestra de la forma\n",
    "\n",
    "$$\n",
    "z_{i1} = \\varphi_{11}x_{i1} + \\varphi_{21}x_{i2} + \\dots + \\varphi_{M1}x_{iM}\n",
    "$$\n",
    "\n",
    "que tiene la mayor varianza de muestra, sujeta a la restricción de que $\\sum_{j=1}^M \\varphi_{j1}^2 = 1$\n",
    "\n",
    "En otras palabras, el primer vector de carga del componente principal resuelve el problema de optimización\n",
    "\n",
    "$$\n",
    "\\max_{\\varphi_{11}, \\dots, \\varphi_{M1}} \\left[ \\frac{1}{N} \\sum_{i=1}^N \\left( \\sum_{j=1}^M \\varphi_{j1}x_{ij} \\right) \\right], \\,\\,\\, \\text{sujeto a} \\,\\,\\, \\sum_{j=1}^M \\varphi_{j1}^2 = 1\n",
    "$$\n",
    "\n",
    "A partir de la ecuación anterior, podemos escribir el objetivo como $\\frac{1}{N}\\sum_{i=1}^Nz_{i1}^2$. Dado que $\\frac{1}{N}\\sum_{i=1}^Nx_{ij} = 0$, el promedio de $z_{11}, \\dots, z_{N1}$ también será cero. Por lo tanto, el objetivo que estamos maximizando en (12.3) es simplemente la varianza muestral de los $N$ valores de $z_{i1}$. Nos referimos a $z_{11}, \\dots, z_{N1}$ como las puntuaciones del primer componente principal. Este problema se puede resolver mediante una descomposición propia, una técnica estándar en álgebra lineal, pero los detalles están fuera del alcance de este libro.\n",
    "\n",
    "Hay una buena interpretación geométrica del primer componente principal. El vector de carga (loading) $\\varphi_{1}$ con elementos $\\varphi_{11}, \\varphi_{21}, \\dots, \\varphi_{M1}$ define una dirección en el espacio de características a lo largo de la cual los datos varían más. Si proyectamos los $N$ puntos de datos $x_1, \\dots, x_N$ en esta dirección, los valores proyectados son las puntuaciones del componente principal $z_{11}, \\dots, z_{N1}$ en sí.\n",
    "Después de que se haya determinado el primer componente principal $Z_1$ de las características, podemos encontrar el segundo componente principal $Z_2$. El segundo componente principal es la combinación lineal de $X_1, \\dots, X_M$ que tiene la varianza máxima de todas las combinaciones lineales que no están correlacionadas con $Z_1$. Las puntuaciones del segundo componente principal $z_{12}, z_{22}, \\dots, z_{N2}$ toman la forma\n",
    "\n",
    "$$\n",
    "z_{i2} = \\varphi_{12}x_{i1} + \\varphi_{22}x_{i2} + \\dots + \\varphi_{M2}x_{iM}\n",
    "$$\n",
    "\n",
    "donde $ \\varphi_{2}$ es el vector de carga del segundo componente principal, con elementos $ \\varphi_{12}, \\varphi_{22}, \\dots, \\varphi_{M2}$. Resulta que restringir que $Z_2$ no esté correlacionado con $Z_1$ es equivalente a restringir que la dirección $ \\varphi_2$ sea ortogonal (perpendicular) a la dirección $ \\varphi_{1}$.\n",
    "\n",
    "**Otra interpretación de los componentes principales**\n",
    "\n",
    "En la sección anterior, describimos los vectores de carga de los componentes principales como las direcciones en el espacio de características a lo largo de las cuales los datos varían más, y los puntajes de los componentes principales como proyecciones a lo largo de estas direcciones. Sin embargo, también puede ser útil una interpretación alternativa de los componentes principales: los componentes principales proporcionan superficies lineales de baja dimensión que son las más cercanas a las observaciones. Aquí ampliamos esa interpretación.\n",
    "\n",
    "El primer vector de carga de los componentes principales tiene una propiedad muy especial: es la línea en el espacio M-dimensional que está más cerca de las $N$ observaciones (usando la distancia euclidiana al cuadrado promedio como medida de cercanía). El atractivo de esta interpretación es claro: buscamos una única dimensión de los datos que se encuentre lo más cerca posible de todos los puntos de datos, ya que dicha línea probablemente proporcionará un buen resumen de los datos.\n",
    "\n",
    "La noción de los componentes principales como las dimensiones que están más cerca de las $N$ observaciones se extiende más allá del primer componente principal. Por ejemplo, los dos primeros componentes principales de un conjunto de datos abarcan el plano más cercano a las $N$ observaciones, en términos de distancia euclidiana al cuadrado promedio. En el panel izquierdo de la Figura 12.2 se muestra un ejemplo. Los tres primeros componentes principales de un conjunto de datos abarcan el hiperplano tridimensional más cercano a las $N$ observaciones, y así sucesivamente.\n",
    "\n",
    "Usando esta interpretación, juntos los primeros $P$ vectores de puntuación de componentes principales y los primeros $P$ vectores de carga de componentes principales proporcionan la mejor aproximación P-dimensional (en términos de distancia euclidiana) a la i-ésima observación $x_{ij}$ . Esta representación se puede escribir como\n",
    "\n",
    "$$\n",
    "x_{ij} \\approx \\sum_{k=1}^P z_{ik}\\varphi_{jk}\n",
    "$$\n",
    "\n",
    "Podemos expresar esto de manera más formal escribiendo un problema de optimización. Supongamos que la matriz de datos $\\mathbf{X}$ está centrada en columnas. De todas las aproximaciones de la forma $xij \\approx \\sum_{k=1}^P1a_{ik}b_{jk}$, podríamos preguntar cuál tiene la suma de cuadrados residual más pequeña:\n",
    "\n",
    "$$\n",
    "\\min_{\\textbf{A} \\in \\mathbb{R}^{N \\times P}, \\, \\textbf{B} \\in \\mathbb{R}^{M \\times P}} \\left[ \\sum_{j=1}^M \\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^P a_{ik}b_{jk} \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Aquí, $\\textbf{A}$ es una matriz $N \\times P$ cuyo elemento $(i, k)$ es $a_{ik}$, y $\\textbf{B}$ es un elemento $M \\times P$ cuyo elemento $(j, k)$ es $b_{jk}$.\n",
    "Se puede demostrar que para cualquier valor de $P$, las columnas de las matrices $\\hat{\\textbf{A}}$ y $\\hat{\\textbf{B}}$ que resuelven (12.6) son de hecho\n",
    "los primeros $P$ vectores de puntaje y carga de componentes principales. En otras palabras, si $\\hat{\\mathbf{A}}$ y $\\hat{\\mathbf{B}}$ resuelven (12.6), entonces $\\hat{a}_{ik} = z_{ik}$ y $\\hat{b}_{jk} = \\varphi_{jk}$. Esto significa que el valor más pequeño posible del objetivo en (12.6) es\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^M \\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^P z_{ik}\\varphi_{jk} \\right)^2\n",
    "$$\n",
    "\n",
    "En resumen, juntos, los vectores de puntaje de componentes principales $P$ y los vectores de carga de componentes principales $P$ pueden dar una buena aproximación a los datos cuando $P$ es suficientemente grande. Cuando $P = min(N − 1, M)$, entonces la representación es exacta: $x_{ij} = \\sum_{k=1}^P z_{ik}\\varphi_{jk}$.\n",
    "\n",
    "**La proporción de varianza explicada**\n",
    "\n",
    "Ahora podemos plantearnos una pregunta natural: ¿cuánta información de un conjunto de datos determinado se pierde al proyectar las observaciones sobre los primeros componentes principales? Es decir, ¿cuánta de la varianza de los datos no está contenida en los primeros componentes principales? En términos más generales, nos interesa saber la proporción de varianza explicada (PVE) por cada componente principal. La varianza total presente en un conjunto de datos (suponiendo que las variables se han centrado para tener media cero) se define como\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^M \\text{Var}(X_j) = \\sum_{j=1}^M\\frac{1}{N}\\sum_{i=1}^Nx_{ij}^2\n",
    "$$\n",
    "\n",
    "y la varianza explicada por el késimo componente principal es\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N z_{ik}^2 = \\frac{1}{N}\\sum_{i=1}^N\\left( \\sum_{j=1}^M \\right)^2\n",
    "$$\n",
    "\n",
    "Por lo tanto, la EVP del késimo componente principal viene dada por\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^N z_{ik}^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2} = \\frac{\\sum_{i=1}^N\\left( \\sum_{j=1}^M \\right)^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2}\n",
    "$$\n",
    "\n",
    "El PVE de cada componente principal es una cantidad positiva. Para calcular el PVE acumulado de los primeros $P$ componentes principales, podemos simplemente sumar cada uno de los primeros $P$ PVE. En total, hay $min(N − 1, M)$ componentes principales, y sus PVE suman uno. Se puede demostrar que los primeros $P$ vectores de carga y puntuación del componente principal se pueden interpretar como la mejor aproximación $P$ dimensional a los datos, en términos de suma de cuadrados residuales. Resulta que la varianza de los datos se puede descomponer en la varianza de los primeros M componentes principales más el error cuadrático medio de esta aproximación $P$-dimensional, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\sum_{j=1}^M\\frac{1}{N}\\sum_{i=1}^Nx_{ij}^2}_{\\text{Var. de los datos}} = \\underbrace{\\sum_{k=1}^P\\frac{1}{N}\\sum_{i=1}^Nz_{ik}^2}_{\\text{Var. de los primeros P componentes principales}} + \\underbrace{\\frac{1}{N}\\sum_{j=1}^M\\sum_{i=1}^N \\left(\n",
    "x_{ij} - \\sum_{k=1}^M z_{ik}\\varphi_{jk} \\right)^2}_{\\text{MSE de la aproximación de dimensión M}}\n",
    "$$\n",
    "\n",
    "Dado que el primer término es fijo, vemos que al maximizar la varianza de los primeros $P$ componentes principales, minimizamos el error cuadrático medio de la aproximación de dimensión $P$, y viceversa. Esto explica por qué los componentes principales pueden considerarse equivalentemente como la minimización del error de aproximación o la maximización de la varianza).\n",
    "\n",
    "Además, podemos usar el resultado anterior para ver que el PVE definido en (12.10) es igual a\n",
    "\n",
    "$$\n",
    "1 - \\frac{\\sum_{j=1}^M\\sum_{i=1}^N \\left(x_{ij} - \\sum_{k=1}^M z_{ik}\\varphi_{jk} \\right)^2}{\\sum_{j=1}^M\\sum_{i=1}^N x_{ij}^2} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "$$\n",
    "\n",
    "donde TSS representa la suma total de elementos al cuadrado de $\\mathbf{X}$, y RSS representa la suma residual de cuadrados de la aproximación $P$-dimensional dada por los componentes principales. Recordando la definición de R2, esto significa que podemos interpretar el PVE como el R2 de la aproximación para $X$ dada por los primeros $P$ componentes principales.\n",
    "\n",
    "**Más sobre PCA**\n",
    "\n",
    "**Escalado de las variables**\n",
    "\n",
    "Ya hemos mencionado que antes de realizar el PCA, las variables deben estar centradas para que tengan una media de cero. Además, los resultados obtenidos cuando realizamos el PCA también dependerán de si las variables han sido escaladas individualmente (cada una multiplicada por una constante diferente).\n",
    "\n",
    "**Unicidad de los componentes principales**\n",
    "\n",
    "Si bien en teoría los componentes principales no necesitan ser únicos, en casi todos los entornos prácticos lo son (hasta que se inviertan los signos). Esto significa que dos paquetes de software diferentes producirán los mismos vectores de carga de componentes principales, aunque los signos de esos vectores de carga pueden diferir. Los signos pueden diferir porque cada vector de carga de componente principal especifica una dirección en el espacio $M$-dimensional: invertir el signo no tiene efecto ya que la dirección no cambia. De manera similar, los vectores de puntuación son únicos hasta que se invierta un signo, ya que la varianza de $Z$ es la misma que la varianza de $−Z$. Vale la pena señalar que cuando usamos la ecuación para aproximar $x_{ij}$ multiplicamos $z_{ik} por \\varphi_{jk}$. Por lo tanto, si el signo se cambia tanto en el vector de carga como en el de puntuación, el producto final de las dos cantidades no cambia.\n",
    "\n",
    "**Decidir cuántos componentes principales utilizar**\n",
    "\n",
    "En general, una matriz de datos $N \\times M$ $\\mathbf{X}$ tiene $\\min(N − 1, M)$ componentes principales distintos. Sin embargo, por lo general no nos interesan todos ellos; más bien, nos gustaría utilizar solo los primeros componentes principales para visualizar o interpretar los datos. De hecho, nos gustaría utilizar la menor cantidad de componentes principales necesarios para comprender bien los datos. ¿Cuántos componentes principales se necesitan? Desafortunadamente, no hay una respuesta única (¡o simple!) a esta pregunta.\n",
    "\n",
    "Por lo general, decidimos la cantidad de componentes principales necesarios para visualizar los datos examinando un gráfico de sedimentación. Elegimos el menor número de componentes principales necesarios para explicar una cantidad considerable de la variación en los datos. Esto se hace observando el gráfico de sedimentación y buscando un punto en el que la proporción de varianza explicada por cada componente principal posterior disminuye. Esta disminución se suele denominar codo en el gráfico de sedimentación.\n",
    "\n",
    "Sin embargo, este tipo de análisis visual es inherentemente ad hoc. Desafortunadamente, no existe una forma objetiva y aceptada de decidir cuántos componentes principales son suficientes. De hecho, la cuestión de cuántos componentes principales son suficientes está inherentemente mal definida y dependerá del área de aplicación específica y del conjunto de datos específico. En la práctica, tendemos a observar los primeros componentes principales para encontrar patrones interesantes en los datos. Si no se encuentran patrones interesantes en los primeros componentes principales, es poco probable que otros componentes principales sean de interés. Por el contrario, si los primeros componentes principales son interesantes, normalmente continuamos observando los componentes principales posteriores hasta que no se encuentren más patrones interesantes. Se reconoce que este es un enfoque subjetivo y refleja el hecho de que el PCA se utiliza generalmente como una herramienta para el análisis exploratorio de datos.\n",
    "\n",
    "Por otro lado, si calculamos los componentes principales para su uso en un análisis supervisado, como la regresión de componentes principales, existe una forma simple y objetiva de determinar cuántos componentes principales utilizar: podemos tratar la cantidad de vectores de puntaje de componentes principales que se utilizarán en la regresión como un parámetro de ajuste que se seleccionará mediante validación cruzada o un enfoque relacionado. La simplicidad comparativa de seleccionar la cantidad de componentes principales para un análisis supervisado es una manifestación del hecho de que los análisis supervisados ​​tienden a definirse con mayor claridad y evaluarse de manera más objetiva que los análisis no supervisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVVglnoM60iuQb8Y59tHbX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
